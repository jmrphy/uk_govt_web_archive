---
title: How Does the State Speak about Globalisation? A Quantitative Text-Mining Approach
author:
- name: Justin Murphy
  affiliation: University of Southampton
  email: j.murphy@soton.ac.uk
date: April 2014
abstract: Scholars argue that the concept of "globalisation" is strategically deployed by governments to rationalise their policy choices [@Hay:2011dh]. This article is the first large-scale quantitative assessment of this argument, using text-mining and machine learning techniques to analyze more than 60,000 government web pages. Specifically, this article exploits the newly released United Kingdom Government Web Archive to analyze a random sample of web pages published across the entire UK government web system between 2000 and 2013.

...

```{r setup, include=FALSE, warning=FALSE}
opts_chunk$set(dev=c("pdf"), fig.width=14, fig.height=14, fig.show='hold', fig.lp="fig:", par=TRUE, echo=FALSE, results="hide", message=FALSE, warning=FALSE, error=FALSE)
knit_hooks$set(par=function(before, options, envir){
if (before && options$fig.show!='none') par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3)
}, crop=hook_pdfcrop)  
```


```{r, echo=FALSE, results='hide', message=FALSE}
require(tm)
require(ggplot2)
require(xtable)
require(pander)
require(topicmodels)
require(slam)
setwd("~/Dropbox/gh_projects/uk_govt_web_archive")
load("data/globalisation_dtm.Rdata")
load("data/globalisation_corpus.Rdata")
inspected<-inspect(dtm)
source("~/Dropbox/gh_projects/uk_govt_web_archive/analyses/basic_descriptives.R")
```

## Data

The UK Government Web Archive stores more than 7 terabytes of digital items hosted across the UK government web system since 2000. These items include a wide variety of web pages but also documents, such as Adobe PDF files, hosted within the UK government web system.  

The UK National Archives provides an API for programmatic web access to the entire Web Archive. The API allows users to search the archive with a basic set of search operators, and provides the full text as well as some metadata for all items in the entire archive which match a given search criteria. The returned results are not ordered according to any principle so their ordering can be thought of as random.

Data collection began with an attempt to scrape 150,000 items from the UK Government Web Archive API. Of the 150,000 attempted, 67,000 items were successfully returned before the request timed-out. Of the returned items, about 1,000 were empty due to connection errors. Thus, the sample consists of a corpus containing 66,000 text documents.




### Descriptive Statistics
```{r globalisation_frequency_plot, echo=FALSE, cache=FALSE, fig.cap="Most Frequent Terms in Web Pages Containing \"Globalisation\""}
freq.plot
```

\pagebreak

#### Economic Terms
```{r Econ-Correlations, echo=FALSE, results='asis', fig.align='center', cache=TRUE}
kable(econ.correlations) 
```

\pagebreak

#### Imperative Terms
```{r Threat-Correlations, echo=FALSE, results='asis', fig.align='center', cache=TRUE}
kable(threat.correlations) 
```
\pagebreak

#### Cluster Analysis

In this section, I use *k*-means clustering to partition the corpus of documents into clusters of relatively similar documents. The *k*-means algorithm, also known as Lloyd's algorithm, is a non-parametric technique for partitioning *n* observations into the *k* clusters which minimize within-cluster variance.^[Specifically, within-cluster variance refers to the within-cluster sum of Euclidean distances from the centroids, or simply within-cluster sum of squared error (SSE)]. 

K-means cluster analysis requires the analyst to define *k* in advance. As the number of clusters is typically not known in advance, the analyst executes the algorithm with several different values for *k* and compares the within-cluster sum of squared error for each. The *k* which results in the clusters with the lowest SSE, or is not significantly improved by additional *k*, is selected as the optimal *k*. This procedure showed that the 66,400 documents are optimally partitioned into about 200 clusters.^[See the Appendix.]

\pagebreak

# Appendix

## Diagnostics

```{r Document-Lengths, echo=FALSE, cache=FALSE, fig.cap="Document Lengths"}
hist<-as.data.frame(apply(dtm, 1, sum))
ggplot(hist, aes(x=hist[,1])) + geom_histogram() + theme_bw(base_size = 22) +
  labs(x="Count", y="Number of Terms in Document", title="The Distribution of Document Length")
```

```{r Cluster-Diagnostics, echo=FALSE, fig.cap="Diagnostic Plot for Finding Optimal K in K-Cluster Analysis"}
source("~/Dropbox/gh_projects/uk_govt_web_archive/analyses/cluster_diagnose_k.R")
wss.plot
```

```{r LDA-Diagnostics, echo=FALSE, fig.cap="Diagnostic Plot for Finding Optimal Number of Topics in LDA Topic Model"}
source("~/Dropbox/gh_projects/uk_govt_web_archive/analyses/lda/lda_diagnose_k.R")
perplexity.plot
```

\pagebreak

# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent
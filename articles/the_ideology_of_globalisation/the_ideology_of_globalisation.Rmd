---
title: How Does the State Speak about Globalisation? A Quantitative Text-Mining Approach
author:
- name: Justin Murphy
  affiliation: University of Southampton
  email: j.murphy@soton.ac.uk
date: April 2014
abstract: Scholars argue that the concept of "globalisation" is strategically deployed by governments to rationalise their actions [@Hay:2011dh]. This article is the first large-scale quantitative assessment of this argument, using text-mining and machine learning techniques to analyze more than 60,000 government web pages. Specifically, this article exploits the newly released United Kingdom Government Web Archive to analyze a random sample of web pages published across the entire UK government web system between 2000 and 2013.

...

```{r setup, include=FALSE, warning=FALSE}
opts_chunk$set(fig.width=8, fig.height=7, fig.show='hold', echo=FALSE, message=FALSE, warning=FALSE, error=FALSE) 
```


```{r, echo=FALSE, results='hide', message=FALSE}
require(tm)
require(ggplot2)
require(xtable)
require(pander)
require(topicmodels)
require(slam)
setwd("~/Dropbox/gh_projects/uk_govt_web_archive")
load("data/globalisation_dtm.Rdata")
load("data/globalisation_corpus.Rdata")
inspected <- inspect(dtm)  # inspect the dtm
```
I requested 150,000 web pages, received 67k and about 1k were errors. Thus, the final sample consists of a corpus of dtm

\pagebreak


### Descriptive Statistics
```{r globalisation_frequency_plot, echo=FALSE, cache=TRUE, fig.cap="Most frequent terms"}
frequencies<-head(sort(colSums(inspected), decreasing=TRUE), n=50) # list terms in order of frequency, with counts
terms<-names(frequencies)
freqs<-data.frame(terms,frequencies)
freqs$terms <- factor(freqs$terms, levels=freqs$terms[order(frequencies)], ordered=TRUE)
options(scipen=999)
ggplot(data=freqs, aes(x=terms, y=frequencies)) +
  geom_point(stat="identity") +
  coord_flip() +
  theme_bw()
```

\pagebreak

#### Correlated terms
```{r Term-Associations, echo=FALSE, results='asis', fig.align='center', cache=TRUE}
assocs<-as.data.frame(findAssocs(dtm, 'global', 0.6))
assocs<-data.frame(row.names(assocs), assocs$global)
names(assocs)<-c("Terms","Correlation")
kable(assocs)
```

#### Cluster Analysis

In this section, I use *k*-means clustering to partition the corpus of documents into clusters of relatively similar documents. The *k*-means algorithm, also known as Lloyd's algorithm, is a non-parametric technique for partitioning *n* observations into the *k* clusters which minimize within-cluster variance.^[Specifically, within-cluster variance refers to the within-cluster sum of Euclidean distances from the centroids, or simply within-cluster sum of squared error (SSE)]. 

K-means cluster analysis requires the analyst to define *k* in advance. As the number of clusters is typically not known in advance, the analyst executes the algorithm with several different values for *k* and compares the within-cluster sum of squared error for each. The *k* which results in the clusters with the lowest SSE, or is not significantly improved by additional *k*, is selected as the optimal *k*. This procedure showed that the 66,400 documents are optimally partitioned into about 200 clusters.^[See the Appendix.]


```{r Cluster-Analysis}

```

\pagebreak

# Appendix

## Diagnostics

```{r Document-Lengths, echo=FALSE, cache=TRUE, fig.cap="Document lengths"}
hist<-as.data.frame(apply(dtm, 1, sum))
ggplot(hist, aes(x=hist[,1])) + geom_histogram() + theme_bw() +
  labs(x="Count", y="Number of Terms in Document", title="The Distribution of Document Length")
```

```{r Cluster-Diagnostics, echo=FALSE}
source("~/Dropbox/gh_projects/uk_govt_web_archive/analyses/cluster_diagnose_k.R")
wss.plot
```


\pagebreak

# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent